[api]
enabled = true

[sources.nginx_logs]
type = "file"
include = ["/var/log/nginx/tracker_access.log"]
read_from = "beginning"

# =====================================================
# TRANSFORM CONFIGURATION - PARSES NGINX LOGS
# =====================================================
[transforms.parse_nginx]
type = "remap"
inputs = ["nginx_logs"]
source = '''
  # parse tracker_log format directly as defined in nginx.conf
  parsed, err = parse_csv(.message, "|")

  if err != null {
    log("Unable to parse nginx log line: " + err, level: "error")
  } else {
    # map the parsed fields to structured JSON matching the nginx tracker_log format
    .@timestamp = parsed[0]
    .request_method = parsed[1]
    .uri = parsed[2]
    if parsed[3] != "" && parsed[3] != "-"  {
      .query_string = parse_query_string!(parsed[3])
    }
    .status = to_int!(parsed[4])
    .request_completion = parsed[5]
    .request_id = parsed[6]

    # no value is indicated by "-" in the nginx logs, only add fields with values
    # to make querying in ES easier and to not mess with the autodetected data type in its index
    if parsed[7] != "-" {
      .referer = parsed[7]
    }
    if parsed[8] != "-" {
      .user_agent = parsed[8]
    }
    .bytes_sent = to_int!(parsed[9])
    .body_bytes_sent = to_int!(parsed[10])
    .connection = to_int!(parsed[11])
    .connection_requests = to_int!(parsed[12])
    # time in seconds with a milliseconds resolution
    .connection_time_ms = to_float!(parsed[13]) * 1000
    .request_time_ms = to_float!(parsed[14]) * 1000
    .upstream_connect_time_ms = to_float!(parsed[15]) * 1000
    .upstream_header_time_ms = to_float!(parsed[16]) * 1000
    .upstream_response_time_ms = to_float!(parsed[17]) * 1000

    if parsed[18] != "-" {
      .sessionid_hash = parsed[18]
    }

    # add common Elasticsearch metadata fields
    .type = "tracker_access"
}

del(.host)
'''

# =====================================================
# SINK CONFIGURATION FOR ELASTICSEARCH
# =====================================================
# use this sink when running with the full ELK stack (docker compose --profile logs up)
[sinks.elasticsearch]
type = "elasticsearch"
inputs = ["parse_nginx"]
endpoints = ["https://es01:9200"]
bulk.action = "create"
bulk.index = "nginx-logs-%Y.%m.%d"
compression = "gzip"
# elasticsearch authentication (required for Elasticsearch 9.x)
auth.strategy = "basic"
auth.user = "elastic"
auth.password = "${ELASTIC_PASSWORD}"
# SSL configuration (required for secure Elasticsearch setup)
tls.ca_file = "/etc/vector/certs/ca/ca.crt"
tls.verify_certificate = true
tls.verify_hostname = true

# =====================================================
# LOCAL DEBUGGING SINK CONFIGURATION
# =====================================================
# comment out the elasticsearch sink above and uncomment this section
# [sinks.console]
# type = "console"
# inputs = ["parse_nginx"]
# encoding.codec = "json"
# target = "stdout"
