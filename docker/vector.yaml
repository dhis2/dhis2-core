# TODO configure and parse access logs using grok
# Elasticsearchâ€™s Data streams feature requires Vector to be configured with the create bulk.action. This is not enabled by default.
[sources.nginx_logs]
type = "file"
include = ["/var/log/nginx/tracker_access.log"] # Adjust the path to your actual log file
read_from = "beginning"

[transforms.parse_nginx]
type = "remap"
inputs = ["nginx_logs"]
source = '''
# Parse using the Grok pattern that matches your log_format
parsed_log = parse_regex!(.message, r'^\\[(.*?)\\] (.*?) (.*?) (\\d+) "(.*?)" "(.*?)" (\\d+) (\\d+) (.*?) - (\\d+) (\\d+)$')

# Map the parsed fields to structured JSON
.timestamp = parsed_log[1]
.uri = parsed_log[2]
.query_string = parsed_log[3]
.status = to_int!(parsed_log[4])
.referer = parsed_log[5]
.user_agent = parsed_log[6]
.bytes_sent = to_int!(parsed_log[7])
.body_bytes_sent = to_int!(parsed_log[8])
.request_time = to_float!(parsed_log[9])
.connection = to_int!(parsed_log[10])
.connection_requests = to_int!(parsed_log[11])

# Parse timestamp into a proper timestamp
.@timestamp = parse_timestamp!(.timestamp, format: "%d/%b/%Y:%H:%M:%S %z")
del(.timestamp)

# Add common Elasticsearch metadata fields
.type = "nginx_access"
.message = .message
'''

[sinks.elasticsearch]
type = "elasticsearch"
inputs = ["parse_nginx"]
endpoint = "http://es01:9200"
index = "nginx-logs-%Y.%m.%d"
compression = "gzip"

# Elasticsearch authentication if needed
# auth.strategy = "basic"
# auth.user = "elastic"
# auth.password = "password"
